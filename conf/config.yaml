defaults:
  - _self_
  - model: llama

seed: 42
dataset_name: meta_math
dry_run: false

wandb:
  project: "lora_initialize_hessian"
  name: null
  entity: "yoonji0821-myongji-university"

peft:
  use_peft: true
  use_rslora: false
  lora_r: 8
  lora_relative_r: null
  lora_target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  train_embeddings: false
  lora_alpha: 16
  lora_dropout: 0.0
  use_loraplus: false
  loraplus_lr_ratio: 1.0
  dora: false
  adalora: false

init:
  mode: hessian
  bsz: 1
  iters: 1
  max_length: 512
  scale: gd        # 선택
  stable_gamma: 16.0

model:
  weight_decay: 0.0
